---
title: "Parallel Computing Practical"
format: html
editor: visual
---

**SetUp**\

```{r}

#All the libraries used in this practical
library(foreach)
library(doParallel)
library(MASS)
library(tibble)
library(iterators)

```

***Question 1***

```{r}



B <- 100 # size of random exponential numbers

Q1 <- foreach(i=1:B, .combine = "rbind") %do% {
      rs_exp <- rexp(B, 1) # generating exponential random variables
      mean_results <- mean(rs_exp) 
      variance_results  <- var(rs_exp)
      c(mean = mean_results,variance = variance_results)
}
```

***Question 2***

*Serial Processing time:*

```{r}

# System time for the serial processing time

(serial_time <- system.time({
  serial_results <- foreach(i = 1:1000, .combine = c) %do% {
    # Bootstrap sample for each iteration
    sample_data <- sample(galaxies, replace = TRUE)  
    median(sample_data)  # Compute median
  }
}))
```

*Parallel Processing:*

```{r}

# Only use 1 less the total number of cores for the laptop 
cores <- detectCores() - 1

# Create and register parallel cluster

c1 <- makeCluster(cores)
registerDoParallel(c1)

# System time for the parallel processing time

(parallel_time <- system.time({
  parallel_results <- foreach(i = 1:1000, .combine = c, .packages = "MASS")%dopar% {
    sample_data <- sample(galaxies, replace = TRUE)  
    median(sample_data)  
  }
}))


stopCluster(c1)
```

Comparing the processing time between serial and parallel processing :\

```{r}
cat("Serial Processing Time: ", serial_time["elapsed"], "seconds\n")
cat("Parallel Processing Time: ", parallel_time["elapsed"], "seconds\n")

```

The parallel process has a higher processing time than the serial process. This might be due to the small data size, where the overhead of managing parallel tasks outweighs the benefits of parallel execution.

*Experiment with Larger Chunks:*

```{r}

# Create and register parallel cluster
c2 <- makeCluster(cores)
registerDoParallel(c2)


# Grouping into chunks of 1000 bootstrap samples
parallel_time_chunked <- system.time({
  parallel_results_chunked <- foreach(i = 1:100, .combine = c, .packages = "MASS") %dopar% {
    # Compute 10 medians per iteration
    replicate(10, median(sample(galaxies, replace = TRUE)))  
  }
})

stopCluster(c2)


print(parallel_time_chunked)

cat("Chunked Parallel Processing Time: ", parallel_time_chunked["elapsed"], "seconds\n")
```

With larger chunks, the parallel process performs better than the serial processing, likely due to reduced overhead in task distribution and communication. The parallel processes that use replicate with more than 1 replication distribute work more efficiently.

***Question 3***

Estimate coverage of a percentile bootstrap confidence interval for the following scenario: sample of size 50 from an exponential distribution with mean 1.

```{r}


sample_mean <- 1

q3_data <- rexp(50,1)

# Generate 10000 samples 
boot_means <- foreach(i = 1:10000, .combine = "c") %do%{
     
     mean(sample(q3_data, 50, replace = TRUE))
     
}


# 95% percentile CI
  quantile1 <- quantile(boot_means, 0.025)
  quantile3 <- quantile(boot_means, 0.975)
  
  # Compute the unbiased CI
unbiased_CI <- c(
  lower_CI = -quantile3 + 2 * sample_mean,
  upper_CI = -quantile1 + 2 * sample_mean
)


cat("95% Unbiased Confidence Interval:\n")


# Create a tibble for better formatting
unbiased_CI_tbl <- tibble(
   "lower_CI" =round( -quantile3 + 2 * sample_mean,3),
  "upper_CI" =round( -quantile1 + 2 * sample_mean,3)
)

# Print as a table
unbiased_CI_tbl
```

***Question 4***

```{r}

set.seed(1234)

# Create the iterator object

iter_data <- irnorm(n = 3, mean = 0, sd = 1, count = 5)

# foreach vector compute the maximum values
max_values <- foreach(i = 1:3, .combine = c) %do%{
  max(iterators::nextElem(iter_data))
}

max_values
```
